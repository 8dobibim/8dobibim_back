# ğŸ—ï¸ ì‹¤ì œ í”„ë¡œì íŠ¸ ê¸°ë°˜ Terraform êµ¬ì„± ê°€ì´ë“œ

> **â±ï¸ ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 60-90ë¶„  
> **ğŸ’¡ ë‚œì´ë„**: ì¤‘ê¸‰  
> **ğŸ“‹ ëª©í‘œ**: Terraformì„ ì‚¬ìš©í•˜ì—¬ AWS EKS í´ëŸ¬ìŠ¤í„°ì™€ ì• í”Œë¦¬ì¼€ì´ì…˜ ìŠ¤íƒ êµ¬ì¶•  
> **ğŸ“ ì „ì²´ ì½”ë“œ**: [GitHubì—ì„œ í™•ì¸í•˜ê¸°](https://github.com/8dobibim/combined/tree/main/8dobibim_back/terraform-related/AWS_terraform_grafana)

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
terraform-related/
â””â”€â”€ AWS_terraform_grafana/
    â”œâ”€â”€ main.tf           # ë©”ì¸ ë¦¬ì†ŒìŠ¤ ì •ì˜
    â”œâ”€â”€ variables.tf      # ë³€ìˆ˜ ì„ ì–¸
    â”œâ”€â”€ versions.tf       # í”„ë¡œë°”ì´ë” ë²„ì „
    â”œâ”€â”€ terraform.tfvars  # ë³€ìˆ˜ ê°’ (Git ì œì™¸)
    â””â”€â”€ tfvars.example    # ë³€ìˆ˜ ì˜ˆì‹œ íŒŒì¼
```

---

## ğŸ”§ Step 1: í”„ë¡œë°”ì´ë” ë° ë²„ì „ ì„¤ì •

### versions.tf
```hcl
terraform {
  required_providers {
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "2.23.0"
    }
    aws = {
      source  = "hashicorp/aws"
      version = "5.45.0"
    }
  }
}
```

**ğŸ” ì„¤ëª…**:
- AWSì™€ Kubernetes í”„ë¡œë°”ì´ë” ì‚¬ìš©
- ë²„ì „ì„ ê³ ì •í•˜ì—¬ ì¼ê´€ì„± ë³´ì¥

---

## ğŸŒ Step 2: ë„¤íŠ¸ì›Œí¬ ì¸í”„ë¼ êµ¬ì„±

### 2-1. í”„ë¡œë°”ì´ë” ì„¤ì •
```hcl
provider "aws" {
  region  = var.aws_region
  profile = "llm"  # AWS CLI í”„ë¡œíŒŒì¼ ì‚¬ìš©
}

# EKS ì¸ì¦ ì •ë³´
data "aws_eks_cluster_auth" "main" {
  name = aws_eks_cluster.main.name
}

# Kubernetes í”„ë¡œë°”ì´ë” (EKS ì—°ë™)
provider "kubernetes" {
  alias                  = "eks"
  host                   = aws_eks_cluster.main.endpoint
  cluster_ca_certificate = base64decode(aws_eks_cluster.main.certificate_authority[0].data)
  token                  = data.aws_eks_cluster_auth.main.token
}
```

**ğŸ” ì„¤ëª…**:
- `profile`: AWS CLIì— ì„¤ì •ëœ í”„ë¡œíŒŒì¼ ì‚¬ìš©
- EKS í´ëŸ¬ìŠ¤í„° ìƒì„± í›„ ìë™ìœ¼ë¡œ ì¸ì¦ ì •ë³´ ê°€ì ¸ì˜´
- Kubernetes í”„ë¡œë°”ì´ë”ëŠ” EKS ì—”ë“œí¬ì¸íŠ¸ì™€ ì—°ë™

### 2-2. VPC ìƒì„±
```hcl
resource "aws_vpc" "main" {
  cidr_block = "10.38.0.0/16"

  tags = {
    Name = "llm-project-eks-vpc"
  }
}
```

**ğŸ” ì„¤ëª…**:
- ì „ìš© VPC ìƒì„± (10.38.0.0/16 ëŒ€ì—­)
- ì¶©ë¶„í•œ IP ì£¼ì†Œ ê³µê°„ í™•ë³´ (65,536ê°œ)

### 2-3. ì„œë¸Œë„· êµ¬ì„±
```hcl
# ì„œë¸Œë„· 1 (ê°€ìš©ì˜ì—­ 2b)
resource "aws_subnet" "subnet_1" {
  vpc_id                  = aws_vpc.main.id
  cidr_block              = "10.38.1.0/24"
  availability_zone       = "ap-northeast-2b"
  map_public_ip_on_launch = true
  
  tags = {
    Name = "llm-subnet-1"
  }
}

# ì„œë¸Œë„· 2 (ê°€ìš©ì˜ì—­ 2c)
resource "aws_subnet" "subnet_2" {
  vpc_id                  = aws_vpc.main.id
  cidr_block              = "10.38.2.0/24"
  availability_zone       = "ap-northeast-2c"
  map_public_ip_on_launch = true
  
  tags = {
    Name = "llm-subnet-2"
  }
}
```

**ğŸ” ì„¤ëª…**:
- 2ê°œì˜ ê°€ìš©ì˜ì—­ì— ì„œë¸Œë„· ë°°ì¹˜ (ê³ ê°€ìš©ì„±)
- Public IP ìë™ í• ë‹¹ ì„¤ì •
- ê° ì„œë¸Œë„·ì€ 256ê°œì˜ IP ì£¼ì†Œ ì œê³µ

### 2-4. ì¸í„°ë„· ì—°ê²° ì„¤ì •
```hcl
# ì¸í„°ë„· ê²Œì´íŠ¸ì›¨ì´
resource "aws_internet_gateway" "main" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = "llm-project-eks-igw"
  }
}

# ë¼ìš°íŒ… í…Œì´ë¸”
resource "aws_route_table" "main" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = "llm-project-eks-rt"
  }
}

# ì„œë¸Œë„·ê³¼ ë¼ìš°íŒ… í…Œì´ë¸” ì—°ê²°
resource "aws_route_table_association" "subnet_1" {
  subnet_id      = aws_subnet.subnet_1.id
  route_table_id = aws_route_table.main.id
}

resource "aws_route_table_association" "subnet_2" {
  subnet_id      = aws_subnet.subnet_2.id
  route_table_id = aws_route_table.main.id
}
```

**ğŸ” ì„¤ëª…**:
- ì¸í„°ë„· ê²Œì´íŠ¸ì›¨ì´ë¡œ ì™¸ë¶€ í†µì‹  í™œì„±í™”
- ëª¨ë“  ì™¸ë¶€ íŠ¸ë˜í”½(0.0.0.0/0)ì„ IGWë¡œ ë¼ìš°íŒ…

---

## ğŸ”’ Step 3: ë³´ì•ˆ ê·¸ë£¹ ì„¤ì •

### 3-1. EKS í´ëŸ¬ìŠ¤í„° ë³´ì•ˆ ê·¸ë£¹
```hcl
resource "aws_security_group" "eks_cluster_sg" {
  name        = "llm-project-eks-sg"
  description = "Security group for EKS cluster"
  vpc_id      = aws_vpc.main.id

  # VPC ë‚´ë¶€ í†µì‹  í—ˆìš©
  ingress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["10.38.0.0/16"]
  }

  # ëª¨ë“  ì•„ì›ƒë°”ìš´ë“œ í—ˆìš©
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}
```

### 3-2. ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ í¬íŠ¸ ê°œë°©
```hcl
# Grafana í¬íŠ¸ (3000-3001)
resource "aws_security_group_rule" "allow_openwebui_grafana" {
  type              = "ingress"
  from_port         = 3000
  to_port           = 3001
  protocol          = "tcp"
  cidr_blocks       = ["0.0.0.0/0"]
  security_group_id = aws_security_group.eks_cluster_sg.id
}

# Prometheus í¬íŠ¸ (9090)
resource "aws_security_group_rule" "allow_prometheus" {
  type              = "ingress"
  from_port         = 9090
  to_port           = 9090
  protocol          = "tcp"
  cidr_blocks       = ["0.0.0.0/0"]
  security_group_id = aws_security_group.eks_cluster_sg.id
}
```

**ğŸ” ì„¤ëª…**:
- VPC ë‚´ë¶€ëŠ” ëª¨ë“  í¬íŠ¸ í†µì‹  í—ˆìš©
- íŠ¹ì • ì„œë¹„ìŠ¤ í¬íŠ¸ë§Œ ì¸í„°ë„·ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥

---

## ğŸ‘¤ Step 4: IAM ì—­í•  ì„¤ì •

### 4-1. EKS í´ëŸ¬ìŠ¤í„° ì—­í• 
```hcl
resource "aws_iam_role" "eks_cluster_role" {
  name = "llm-project-eks-eks-cluster-role-v3"

  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect    = "Allow",
      Principal = { Service = "eks.amazonaws.com" },
      Action    = "sts:AssumeRole"
    }]
  })
}

# EKS í´ëŸ¬ìŠ¤í„° ì •ì±… ì—°ê²°
resource "aws_iam_role_policy_attachment" "eks_cluster_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
  role       = aws_iam_role.eks_cluster_role.name
}
```

### 4-2. ë…¸ë“œ ê·¸ë£¹ ì—­í• 
```hcl
resource "aws_iam_role" "eks_node_role" {
  name = "llm-project-eks-eks-node-role-v3"

  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect    = "Allow",
      Principal = { Service = "ec2.amazonaws.com" },
      Action    = "sts:AssumeRole"
    }]
  })
}

# í•„ìˆ˜ ì •ì±… ì—°ê²°
resource "aws_iam_role_policy_attachment" "eks_worker_node_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
  role       = aws_iam_role.eks_node_role.name
}

resource "aws_iam_role_policy_attachment" "eks_cni_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
  role       = aws_iam_role.eks_node_role.name
}

resource "aws_iam_role_policy_attachment" "eks_container_registry_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
  role       = aws_iam_role.eks_node_role.name
}

# EBS CSI ë“œë¼ì´ë²„ ì •ì±… (ì˜êµ¬ ìŠ¤í† ë¦¬ì§€ìš©)
resource "aws_iam_role_policy_attachment" "ebs_csi_policy" {
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy"
  role       = aws_iam_role.eks_node_role.name
}
```

**ğŸ” ì„¤ëª…**:
- EKS í´ëŸ¬ìŠ¤í„°ì™€ ë…¸ë“œì— í•„ìš”í•œ ìµœì†Œ ê¶Œí•œë§Œ ë¶€ì—¬
- EBS CSI ë“œë¼ì´ë²„ë¡œ ì˜êµ¬ ë³¼ë¥¨ ì‚¬ìš© ê°€ëŠ¥

---

## âš™ï¸ Step 5: EKS í´ëŸ¬ìŠ¤í„° êµ¬ì„±

### 5-1. EKS í´ëŸ¬ìŠ¤í„° ìƒì„±
```hcl
resource "aws_eks_cluster" "main" {
  name     = var.cluster_name
  role_arn = aws_iam_role.eks_cluster_role.arn

  vpc_config {
    subnet_ids              = var.private_subnet_ids
    security_group_ids      = [aws_security_group.eks_cluster_sg.id]
    endpoint_private_access = true
    endpoint_public_access  = true
  }

  kubernetes_network_config {
    service_ipv4_cidr = "172.20.0.0/16"
  }

  depends_on = [aws_iam_role_policy_attachment.eks_cluster_policy]
}
```

**ğŸ” ì„¤ëª…**:
- Private/Public ì—”ë“œí¬ì¸íŠ¸ ëª¨ë‘ í™œì„±í™”
- ì„œë¹„ìŠ¤ ë„¤íŠ¸ì›Œí¬ëŠ” 172.20.0.0/16 ì‚¬ìš© (VPCì™€ ê²¹ì¹˜ì§€ ì•ŠìŒ)

### 5-2. ë…¸ë“œ ê·¸ë£¹ ìƒì„±
```hcl
resource "aws_eks_node_group" "nodes" {
  cluster_name    = aws_eks_cluster.main.name
  node_group_name = "${var.cluster_name}-nodes"
  node_role_arn   = aws_iam_role.eks_node_role.arn
  subnet_ids      = var.private_subnet_ids
  instance_types  = [var.instance_type]
  
  scaling_config {
    desired_size = var.node_group_desired_capacity
    min_size     = var.node_group_min_capacity
    max_size     = var.node_group_max_capacity
  }

  depends_on = [
    aws_iam_role_policy_attachment.eks_worker_node_policy,
    aws_iam_role_policy_attachment.eks_cni_policy,
    aws_iam_role_policy_attachment.eks_container_registry_policy,
    aws_iam_role_policy_attachment.ebs_csi_policy
  ]
}
```

**ğŸ” ì„¤ëª…**:
- ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì„¤ì • (1~3ê°œ ë…¸ë“œ)
- t3.xlarge ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš© (4 vCPU, 16GB RAM)

### 5-3. EKS ì• ë“œì˜¨ ì„¤ì¹˜
```hcl
resource "aws_eks_addon" "ebs_csi_driver" {
  cluster_name = aws_eks_cluster.main.name
  addon_name   = "aws-ebs-csi-driver"
  
  depends_on = [
    aws_eks_cluster.main,
    aws_iam_role_policy_attachment.ebs_csi_policy
  ]
}
```

---

## ğŸ˜ Step 6: PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì„±

### 6-1. ì˜êµ¬ ë³¼ë¥¨ í´ë ˆì„
```hcl
resource "kubernetes_persistent_volume_claim" "postgres_pvc" {
  provider   = kubernetes.eks
  depends_on = [
    aws_eks_node_group.nodes,
    aws_eks_addon.ebs_csi_driver
  ]

  metadata {
    name      = "postgres-pvc"
    namespace = var.namespace
  }

  spec {
    access_modes = ["ReadWriteOnce"]
    resources {
      requests = { storage = "5Gi" }
    }
    storage_class_name = "gp2-immediate"
  }
}
```

### 6-2. PostgreSQL ë°°í¬
```hcl
resource "kubernetes_deployment" "postgres_deployment" {
  provider   = kubernetes.eks
  depends_on = [
    aws_eks_node_group.nodes,
    kubernetes_persistent_volume_claim.postgres_pvc
  ]

  metadata {
    name      = "postgres"
    namespace = var.namespace
    labels = {
      app = "postgres"
    }
  }

  spec {
    replicas = 1
    selector {
      match_labels = {
        app = "postgres"
      }
    }

    template {
      metadata {
        labels = {
          app = "postgres"
        }
      }

      spec {
        container {
          name  = "postgres"
          image = "postgres:13"

          env {
            name  = "POSTGRES_DB"
            value = var.postgres_db
          }
          env {
            name  = "POSTGRES_USER"
            value = var.postgres_user
          }
          env {
            name  = "POSTGRES_PASSWORD"
            value = var.postgres_password
          }
          env {
            name  = "PGDATA"
            value = "/var/lib/postgresql/data/pgdata"
          }
          
          port {
            container_port = 5432
          }

          volume_mount {
            mount_path = "/var/lib/postgresql/data"
            name       = "postgres-storage"
          }
        }

        volume {
          name = "postgres-storage"
          persistent_volume_claim {
            claim_name = "postgres-pvc"
          }
        }
      }
    }
  }
}
```

### 6-3. PostgreSQL ì„œë¹„ìŠ¤
```hcl
resource "kubernetes_service" "postgres_service" {
  provider   = kubernetes.eks
  depends_on = [aws_eks_node_group.nodes]

  metadata {
    name      = "postgres-service"
    namespace = var.namespace
  }

  spec {
    selector = {
      app = "postgres"
    }
    port {
      port        = 5432
      target_port = 5432
    }
    type = "ClusterIP"
  }
}
```

**ğŸ” ì„¤ëª…**:
- 5GB EBS ë³¼ë¥¨ìœ¼ë¡œ ë°ì´í„° ì˜êµ¬ ì €ì¥
- ClusterIPë¡œ ë‚´ë¶€ í†µì‹ ë§Œ í—ˆìš©
- PGDATA ê²½ë¡œë¥¼ ì„œë¸Œë””ë ‰í† ë¦¬ë¡œ ì„¤ì • (ê¶Œí•œ ë¬¸ì œ ë°©ì§€)

---

## ğŸ¤– Step 7: LiteLLM í”„ë¡ì‹œ êµ¬ì„±

### 7-1. LiteLLM ë°°í¬
```hcl
resource "kubernetes_deployment" "litellm_deployment" {
  provider   = kubernetes.eks
  depends_on = [aws_eks_node_group.nodes]

  metadata {
    name      = "litellm"
    namespace = var.namespace
    labels = {
      app = "litellm"
    }
  }
  
  spec {
    replicas = 1
    selector {
      match_labels = {
        app = "litellm"
      }
    }
    
    template {
      metadata {
        labels = {
          app = "litellm"
        }
      }
      
      spec {
        container {
          name  = "litellm"
          image = "ghcr.io/berriai/litellm:main"
          
          # Azure OpenAI ì„¤ì •
          env {
            name  = "AZURE_API_KEY"
            value = var.AZURE_API_KEY
          }
          env {
            name  = "AZURE_OPENAI_API_KEY"
            value = var.AZURE_API_KEY
          }
          env {
            name  = "AZURE_API_BASE"
            value = var.AZURE_API_BASE
          }
          env {
            name  = "AZURE_API_VERSION"
            value = var.AZURE_API_VERSION
          }
          
          # Gemini API ì„¤ì •
          env {
            name  = "GEMINI_API_KEY"
            value = var.GEMINI_API_KEY
          }
          
          # ë°ì´í„°ë² ì´ìŠ¤ ë° ì¸ì¦
          env {
            name  = "DATABASE_URL"
            value = var.database_url
          }
          env {
            name  = "LITELLM_MASTER_KEY"
            value = var.litellm_master_key
          }
          env {
            name  = "LITELLM_SALT_KEY"
            value = var.litellm_salt_key
          }
          
          port {
            container_port = var.litellm_api_port      # 4000
          }
          port {
            container_port = var.litellm_metrics_port  # 8000
          }
        }
      }
    }
  }
}
```

### 7-2. LiteLLM ì„œë¹„ìŠ¤ (LoadBalancer)
```hcl
resource "kubernetes_service" "litellm_service" {
  provider   = kubernetes.eks
  depends_on = [aws_eks_node_group.nodes]

  metadata {
    name      = "litellm-service"
    namespace = var.namespace
    annotations = {
      "service.beta.kubernetes.io/aws-load-balancer-type"   = "nlb"
      "service.beta.kubernetes.io/aws-load-balancer-scheme" = "internet-facing"
    }
  }
  
  spec {
    selector = {
      app = "litellm"
    }
    port {
      name        = "api"
      protocol    = "TCP"
      port        = var.litellm_api_port
      target_port = var.litellm_api_port
    }
    port {
      name        = "metrics"
      protocol    = "TCP"
      port        = var.litellm_metrics_port
      target_port = var.litellm_metrics_port
    }
    type = "LoadBalancer"
  }
}
```

**ğŸ” ì„¤ëª…**:
- ë©€í‹° LLM í”„ë¡ì‹œ (Azure OpenAI, Gemini ì§€ì›)
- NLB(Network Load Balancer) ì‚¬ìš©ìœ¼ë¡œ ë‚®ì€ ì§€ì—°ì‹œê°„
- APIì™€ ë©”íŠ¸ë¦­ í¬íŠ¸ ë¶„ë¦¬

---

## ğŸŒ Step 8: OpenWebUI êµ¬ì„±

### 8-1. OpenWebUI ë°°í¬
```hcl
resource "kubernetes_deployment" "openwebui_deployment" {
  provider   = kubernetes.eks
  depends_on = [aws_eks_node_group.nodes]

  metadata {
    name      = "openwebui"
    namespace = var.namespace
    labels = {
      app = "openwebui"
    }
  }
  
  spec {
    replicas = 1
    selector {
      match_labels = {
        app = "openwebui"
      }
    }
    
    template {
      metadata {
        labels = {
          app = "openwebui"
        }
      }
      
      spec {
        container {
          name  = "openwebui"
          image = "ghcr.io/open-webui/open-webui:main"
          command = ["uvicorn", "open_webui.main:app", "--host", "0.0.0.0", "--port", "3000"]
          
          env {
            name  = "DATABASE_URL"
            value = var.database_url
          }
          env {
            name  = "LITELLM_PROXY_BASE_URL"
            value = "http://litellm-service:${var.litellm_api_port}"
          }
          
          port {
            container_port = var.openwebui_port  # 3000
          }
        }
      }
    }
  }
}
```

### 8-2. OpenWebUI ì„œë¹„ìŠ¤
```hcl
resource "kubernetes_service" "openwebui_service" {
  provider = kubernetes.eks
  depends_on = [aws_eks_node_group.nodes]

  metadata {
    name      = "openwebui-service"
    namespace = var.namespace
    annotations = {
      "service.beta.kubernetes.io/aws-load-balancer-type"   = "classic"
      "service.beta.kubernetes.io/aws-load-balancer-scheme" = "internet-facing"
    }
  }
  
  spec {
    selector = {
      app = "openwebui"
    }
    port {
      protocol    = "TCP"
      port        = var.openwebui_port
      target_port = var.openwebui_port
    }
    type = "LoadBalancer"
  }
}
```

---

## ğŸ“Š Step 9: ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ

### 9-1. Prometheus ë°°í¬
```hcl
resource "kubernetes_deployment" "prometheus_deployment" {
  provider   = kubernetes.eks
  depends_on = [aws_eks_node_group.nodes]

  metadata {
    name      = "prometheus"
    namespace = var.namespace
    labels = {
      app = "prometheus"
    }
  }
  
  spec {
    replicas = 1
    selector {
      match_labels = {
        app = "prometheus"
      }
    }
    
    template {
      metadata {
        labels = {
          app = "prometheus"
        }
      }
      
      spec {
        container {
          name  = "prometheus"
          image = "prom/prometheus:latest"
          port {
            container_port = var.prometheus_port  # 9090
          }
        }
      }
    }
  }
}
```

### 9-2. Grafana ë°°í¬
```hcl
resource "kubernetes_deployment" "grafana_deployment" {
  provider   = kubernetes.eks
  depends_on = [aws_eks_node_group.nodes]

  metadata {
    name      = "grafana"
    namespace = var.namespace
    labels = {
      app = "grafana"
    }
  }
  
  spec {
    replicas = 1
    selector {
      match_labels = {
        app = "grafana"
      }
    }
    
    template {
      metadata {
        labels = {
          app = "grafana"
        }
      }
      
      spec {
        container {
          name  = "grafana"
          image = "grafana/grafana:latest"
          port {
            container_port = var.grafana_port  # 3001
          }
        }
      }
    }
  }
}
```

---

## ğŸ“ Step 10: ë³€ìˆ˜ ì„¤ì •

### variables.tf (ì£¼ìš” ë³€ìˆ˜)
```hcl
variable "aws_region" {
  description = "AWS region to deploy resources"
  type        = string
  default     = "ap-northeast-2"
}

variable "cluster_name" {
  description = "Name of the EKS cluster"
  type        = string
  default     = "llm-project-eks"
}

variable "instance_type" {
  description = "EC2 instance type for EKS worker nodes"
  type        = string
  default     = "t3.xlarge"
}

variable "node_group_desired_capacity" {
  description = "Desired number of worker nodes"
  type        = number
  default     = 2
}

# API í‚¤ ë³€ìˆ˜ë“¤
variable "GEMINI_API_KEY" {
  description = "Gemini API key for LiteLLM"
  type        = string
  sensitive   = true
}

variable "AZURE_API_KEY" {
  description = "Azure OpenAI API key"
  type        = string
  sensitive   = true
}

# ë°ì´í„°ë² ì´ìŠ¤ ë³€ìˆ˜ë“¤
variable "postgres_user" {
  description = "PostgreSQL username"
  type        = string
  sensitive   = true
}

variable "postgres_password" {
  description = "PostgreSQL password"
  type        = string
  sensitive   = true
}
```

### terraform.tfvars.example
```hcl
# AWS ì„¤ì •
aws_region = "ap-northeast-2"
cluster_name = "llm-project-eks"

# ë„¤íŠ¸ì›Œí¬ ì„¤ì • (ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€ê²½ í•„ìš”)
vpc_id = "vpc-0f11e0b7cb225055a"
private_subnet_ids = ["subnet-020d530cbcaed0656", "subnet-06bed7cfb33b120c1"]

# ë…¸ë“œ ê·¸ë£¹ ì„¤ì •
instance_type = "t3.medium"
node_group_desired_capacity = 2
node_group_min_capacity = 1
node_group_max_capacity = 3

# ì• í”Œë¦¬ì¼€ì´ì…˜ í¬íŠ¸
litellm_api_port = 4000
litellm_metrics_port = 8000
openwebui_port = 3000
prometheus_port = 9090
grafana_port = 3001

# ë¯¼ê°í•œ ì •ë³´ (ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€ê²½ í•„ìˆ˜!)
# ì´ ê°’ë“¤ì€ í™˜ê²½ ë³€ìˆ˜ë‚˜ CI/CD ì‹œí¬ë¦¿ìœ¼ë¡œ ê´€ë¦¬
litellm_master_key = "<YOUR_MASTER_KEY>"
litellm_salt_key = "<YOUR_SALT_KEY>"
GEMINI_API_KEY = "<YOUR_GEMINI_KEY>"
AZURE_API_KEY = "<YOUR_AZURE_KEY>"
AZURE_API_BASE = "https://your-resource.openai.azure.com/"
AZURE_API_VERSION = "2023-07-01-preview"

# PostgreSQL ì„¤ì •
postgres_user = "openwebui_user"
postgres_password = "<SECURE_PASSWORD>"
postgres_db = "openwebui"
database_url = "postgresql://openwebui_user:<PASSWORD>@postgres-service:5432/openwebui"
```

---

## ğŸš€ ë°°í¬ ì‹¤í–‰ ê°€ì´ë“œ

### 1. ì‚¬ì „ ì¤€ë¹„
```bash
# AWS CLI í”„ë¡œíŒŒì¼ ì„¤ì •
aws configure --profile llm

# ì‘ì—… ë””ë ‰í† ë¦¬ ì´ë™
cd terraform-related/AWS_terraform_grafana
```

### 2. ë³€ìˆ˜ íŒŒì¼ ì¤€ë¹„
```bash
# ì˜ˆì‹œ íŒŒì¼ ë³µì‚¬
cp tfvars.example terraform.tfvars

# ì‹¤ì œ ê°’ìœ¼ë¡œ ìˆ˜ì •
vim terraform.tfvars
```

### 3. Terraform ì‹¤í–‰
```bash
# ì´ˆê¸°í™”
terraform init

# ê³„íš í™•ì¸
terraform plan

# ë°°í¬ ì‹¤í–‰
terraform apply
```

### 4. kubectl ì„¤ì •
```bash
# kubeconfig ì—…ë°ì´íŠ¸
aws eks update-kubeconfig --region ap-northeast-2 --name llm-project-eks --profile llm

# í´ëŸ¬ìŠ¤í„° í™•ì¸
kubectl get nodes
kubectl get pods -A
```

### 5. ì„œë¹„ìŠ¤ URL í™•ì¸
```bash
# LoadBalancer URL í™•ì¸
kubectl get svc -A | grep LoadBalancer

# ê° ì„œë¹„ìŠ¤ë³„ ì—”ë“œí¬ì¸íŠ¸
echo "OpenWebUI: http://$(kubectl get svc openwebui-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'):3000"
echo "Grafana: http://$(kubectl get svc grafana-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'):3001"
echo "Prometheus: http://$(kubectl get svc prometheus-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'):9090"
```

---

## ğŸ’° ë¹„ìš© ìµœì í™” íŒ

### ê°œë°œ í™˜ê²½ ì„¤ì •
```hcl
# terraform.tfvarsì—ì„œ ì¡°ì •
instance_type = "t3.small"              # ì‘ì€ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
node_group_desired_capacity = 1         # ìµœì†Œ ë…¸ë“œë¡œ ìš´ì˜
node_group_min_capacity = 0             # ì•¼ê°„ ìŠ¤ì¼€ì¼ë‹¤ìš´ ê°€ëŠ¥
```

### ë¦¬ì†ŒìŠ¤ ì •ë¦¬
```bash
# ì „ì²´ ë¦¬ì†ŒìŠ¤ ì‚­ì œ
terraform destroy

# íŠ¹ì • ë¦¬ì†ŒìŠ¤ë§Œ ì‚­ì œ
